{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked auto-encoder\n",
    "\n",
    "version = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import json\n",
    "import torch.nn.functional as F  \n",
    "import matplotlib\n",
    "matplotlib.use('Agg') \n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_dict = pd.read_pickle('../B4PPI-main/data/medium_set/embeddings/embeddings_merged.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, protein_dict, max_len=1502):\n",
    "        self.keys = list(protein_dict.keys())\n",
    "        self.protein_dict = protein_dict\n",
    "        self.max_len = max_len\n",
    "        self.scale_factor = 20 \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = self.keys[idx]\n",
    "        seq = self.protein_dict[key]  # shape: (seq_len, 960)\n",
    "        seq_len = seq.shape[0]\n",
    "\n",
    "        need_cleanup = False\n",
    "        # padding to max_len\n",
    "        if seq_len < self.max_len:\n",
    "            pad_size = (self.max_len - seq_len, 960)\n",
    "            padding = torch.zeros(pad_size, dtype=seq.dtype)\n",
    "            seq = torch.cat([seq, padding], dim=0)\n",
    "            need_cleanup = True\n",
    "        else:\n",
    "            seq = seq[:self.max_len]  # truncate if needed\n",
    "        #print(f\"输入数据统计 - 均值: {seq.mean():.4f} 方差: {seq.var():.4f}\")\n",
    "        #均值为0，方差0.0002-0.001之间；说明特征之间相似度过高\n",
    "        seq = seq * self.scale_factor\n",
    "        if need_cleanup:\n",
    "            del padding\n",
    "        return seq.clone()  # shape: (max_len, 960)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mask 75%\n",
    "\n",
    "without taking into account the influence of padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Masked Autoencoder with Transformer\n",
    "class TransformerMAE(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim=960,\n",
    "                 embed_dim=512,\n",
    "                 mask_ratio=0.75,\n",
    "                 num_layers=4,\n",
    "                 nhead=16,\n",
    "                 ff_dim=2048,\n",
    "                 max_len=1502):\n",
    "        super().__init__()\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "        # ---- embed & mask token & pos embed ----\n",
    "        self.embed = nn.Linear(input_dim, embed_dim)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, max_len, embed_dim))\n",
    "\n",
    "        # ---- Transformer encoder ----\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=ff_dim,\n",
    "            batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # ---- decoder head (MLP) ----\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(ff_dim, input_dim)\n",
    "        )\n",
    "\n",
    "        # ---- (embed_dim -> input_dim) ----\n",
    "        self.compress_head = nn.Linear(embed_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor (B, L, 960)\n",
    "        return:\n",
    "          - recon: Tensor (B, L, 960)   # 重建整个序列\n",
    "          - compressed: Tensor (B, 960) # 池化后的压缩向量\n",
    "          - mask_idx: LongTensor (B, num_mask)\n",
    "        \"\"\"\n",
    "        B, L, _ = x.shape\n",
    "        x = self.embed(x)  # (B, L, E)\n",
    "\n",
    "        num_mask = int(L * self.mask_ratio)\n",
    "        noise = torch.rand(B, L, device=x.device)\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)\n",
    "        mask_idx = ids_shuffle[:, :num_mask]\n",
    "\n",
    "        for i in range(B):\n",
    "            x[i, mask_idx[i]] = self.mask_token\n",
    "\n",
    "        x = x + self.pos_embed  # (1,L,E) broadcast to (B,L,E)\n",
    "        enc_out = self.encoder(x)  # (B, L, E)\n",
    "        recon = self.decoder(enc_out)  # (B, L, 960)\n",
    "        pooled = enc_out.mean(dim=1)       # (B, E)\n",
    "        compressed = self.compress_head(pooled)  # (B, 960)\n",
    "\n",
    "        return recon, compressed, mask_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def mae_loss(recon, orig, mask_idx):\n",
    "\n",
    "    pred = recon.gather(1, mask_idx.unsqueeze(-1).expand(-1, -1, recon.size(-1)))\n",
    "    target = orig.gather(1, mask_idx.unsqueeze(-1).expand(-1, -1, orig.size(-1)))\n",
    "    scale_factor = 20\n",
    "    loss = F.huber_loss(pred * scale_factor, target * scale_factor, delta=0.5)\n",
    "    return loss\n",
    "\n",
    "def plot_reconstruction(orig, recon, mask_idx, epoch, batch_idx, ts):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    orig_np = orig[0, :, 0].cpu().numpy().copy() \n",
    "    recon_np = recon[0, :, 0].cpu().numpy().copy()\n",
    "    plt.plot(orig_np, label='Original', alpha=0.7, linewidth=2)\n",
    "    plt.plot(recon_np,'--', label='Reconstructed', linewidth=1.5)\n",
    "    mask_pos = mask_idx[0].numpy()\n",
    "    plt.scatter(mask_pos, orig[0, mask_pos, 0].numpy(),\n",
    "                color='red', marker='x', s=50, label='Masked Positions')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(f\"Epoch {epoch} Batch {batch_idx} - Reconstruction\")\n",
    "    plt.xlabel('Sequence Position')\n",
    "    plt.ylabel('Feature Value')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    os.makedirs(f'logs/recon_plots_{ts}', exist_ok=True)\n",
    "    plt.savefig(f'logs/recon_plots_{ts}/epoch{epoch}_batch{batch_idx}.png')\n",
    "    plt.close()\n",
    "    del orig_np, recon_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(protein_dict):\n",
    "    dataset = ProteinDataset(protein_dict)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2,persistent_workers=True,  # for cpu limitation\n",
    "    pin_memory=False)   \n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = TransformerMAE().to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=1,eta_min=1e-5)\n",
    "\n",
    "    os.makedirs('logs', exist_ok=True)\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    ts = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    log_path = f'logs/mae_train_{ts}.json'\n",
    "    best_path = f'models/mae_best_{ts}.pth'\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    history = []\n",
    "\n",
    "    epochs = 80\n",
    "    all_epoch_losses=[]\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            batch = batch.to(device).float()  # (B, L, 960)\n",
    "            with torch.cuda.amp.autocast(enabled=False):  \n",
    "                recon, compressed, mask_idx = model(batch)\n",
    "            loss = mae_loss(recon, batch, mask_idx)\n",
    "            del recon, compressed\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)  \n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f'Epoch {epoch}/{epochs}  Batch {batch_idx}  Loss {loss.item():.4f}')\n",
    "            if batch_idx == 200 : \n",
    "                with torch.no_grad():\n",
    "                    sample = batch[:1].to(device)  \n",
    "                    recon, _, mask_idx = model(sample)\n",
    "                    \n",
    "                    plot_reconstruction(\n",
    "                        sample.cpu(), \n",
    "                        recon.cpu(), \n",
    "                        mask_idx.cpu(),\n",
    "                        epoch=epoch,\n",
    "                        batch_idx=batch_idx,\n",
    "                        ts=ts\n",
    "                    )\n",
    "\n",
    "        epoch_loss = np.mean(losses)\n",
    "        history.append(epoch_loss)\n",
    "\n",
    "        # save\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_loss\n",
    "            }, best_path)\n",
    "            print(f'>>> 新最佳模型，Epoch={epoch}  Loss={best_loss:.4f}')\n",
    "\n",
    "        with open(log_path, 'a') as f:\n",
    "            log = {\n",
    "                'epoch': epoch,\n",
    "                'loss': epoch_loss,\n",
    "                'best_loss': best_loss,\n",
    "                'time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "            f.write(json.dumps(log) + '\\n')\n",
    "\n",
    "        print(f'--- Epoch {epoch} 完成，Avg Loss={epoch_loss:.4f}，Best Loss={best_loss:.4f} ---')\n",
    "        all_epoch_losses.append(epoch_loss)\n",
    "        gc.collect()        \n",
    "        torch.cuda.empty_cache()  \n",
    "        \n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(all_epoch_losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training Loss Curve (Current: {epoch_loss:.4f})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'logs/loss_curve_{ts}.png') \n",
    "    plt.close()  \n",
    "\n",
    "    final_path = f'models/mae_final_{ts}.pth'\n",
    "    torch.save({\n",
    "        'epoch': epochs,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': history[-1]\n",
    "    }, final_path)\n",
    "    print(f'训练结束，最终模型已保存到 {final_path}')\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_152267/2388880182.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):  # 禁用混合精度（CPU无用）\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10  Batch 0  Loss 0.1677\n",
      ">>> 新最佳模型，Epoch=1  Loss=0.1197\n",
      "--- Epoch 1 完成，Avg Loss=0.1197，Best Loss=0.1197 ---\n",
      "Epoch 2/10  Batch 0  Loss 0.1188\n",
      ">>> 新最佳模型，Epoch=2  Loss=0.1087\n",
      "--- Epoch 2 完成，Avg Loss=0.1087，Best Loss=0.1087 ---\n",
      "Epoch 3/10  Batch 0  Loss 0.0997\n",
      ">>> 新最佳模型，Epoch=3  Loss=0.1072\n",
      "--- Epoch 3 完成，Avg Loss=0.1072，Best Loss=0.1072 ---\n",
      "Epoch 4/10  Batch 0  Loss 0.1199\n",
      ">>> 新最佳模型，Epoch=4  Loss=0.1059\n",
      "--- Epoch 4 完成，Avg Loss=0.1059，Best Loss=0.1059 ---\n",
      "Epoch 5/10  Batch 0  Loss 0.0986\n",
      "--- Epoch 5 完成，Avg Loss=0.1060，Best Loss=0.1059 ---\n",
      "Epoch 6/10  Batch 0  Loss 0.0986\n",
      "--- Epoch 6 完成，Avg Loss=0.1062，Best Loss=0.1059 ---\n",
      "Epoch 7/10  Batch 0  Loss 0.1033\n",
      "--- Epoch 7 完成，Avg Loss=0.1060，Best Loss=0.1059 ---\n",
      "Epoch 8/10  Batch 0  Loss 0.0932\n",
      "--- Epoch 8 完成，Avg Loss=0.1068，Best Loss=0.1059 ---\n",
      "Epoch 9/10  Batch 0  Loss 0.1212\n",
      ">>> 新最佳模型，Epoch=9  Loss=0.1054\n",
      "--- Epoch 9 完成，Avg Loss=0.1054，Best Loss=0.1054 ---\n",
      "Epoch 10/10  Batch 0  Loss 0.0927\n",
      ">>> 新最佳模型，Epoch=10  Loss=0.1042\n",
      "--- Epoch 10 完成，Avg Loss=0.1042，Best Loss=0.1042 ---\n",
      "训练结束，最终模型已保存到 models/mae_final_20250527-165911.pth\n"
     ]
    }
   ],
   "source": [
    "history = train(protein_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载模型：models/mae_best_20250524-003857.pth (Epoch 60, Loss 1.0562)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerMAE(\n",
       "  (embed): Linear(in_features=960, out_features=512, bias=True)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=2048, out_features=960, bias=True)\n",
       "  )\n",
       "  (compress_head): Linear(in_features=512, out_features=960, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "def load_model(model, optimizer, model_path):\n",
    "    checkpoint = torch.load(model_path, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    return model, optimizer, epoch, loss\n",
    "\n",
    "\n",
    "model_path = 'models/mae_best_20250524-003857.pth' \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TransformerMAE().to(device).float()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "model, optimizer, epoch, loss = load_model(model, optimizer, model_path)\n",
    "\n",
    "print(f\"加载模型：{model_path} (Epoch {epoch}, Loss {loss:.4f})\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 1502      \n",
    "scale_factor = 20   \n",
    "batch_size = 32     \n",
    "\n",
    "new_dict = {}\n",
    "keys = list(protein_dict.keys())\n",
    "\n",
    "for start_idx in range(0, len(keys), batch_size):\n",
    "    batch_keys = keys[start_idx:start_idx + batch_size]\n",
    "    batch_seqs = [protein_dict[key] for key in batch_keys]\n",
    "    \n",
    "    processed_batch = []\n",
    "    for seq in batch_seqs:\n",
    "        seq_len = seq.shape[0]\n",
    "        \n",
    "        if isinstance(seq, torch.Tensor):\n",
    "            seq = seq.float()  # 转换为float32\n",
    "        else:\n",
    "            seq = torch.tensor(seq, dtype=torch.float32)  # numpy数组转tensor\n",
    "        \n",
    "        if seq_len < max_len:\n",
    "            pad = torch.zeros((max_len - seq_len, 960), dtype=torch.float32)\n",
    "            processed_seq = torch.cat([seq, pad], dim=0)\n",
    "        else:\n",
    "            processed_seq = seq[:max_len]\n",
    "        \n",
    "        processed_batch.append(processed_seq * scale_factor)\n",
    "    \n",
    "    input_tensor = torch.stack(processed_batch).to(device).to(torch.float32)  # 显式指定类型\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, compressed, _ = model(input_tensor)\n",
    "    \n",
    "    compressed_np = compressed.cpu().numpy()\n",
    "    for key, vec in zip(batch_keys, compressed_np):\n",
    "        new_dict[key] = vec  # vec形状为(960,)\n",
    "\n",
    "    del input_tensor, compressed\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('compressed_protein_features_fit01.pkl', 'wb') as f:\n",
    "    pickle.dump(new_dict, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/richard/miniconda3/envs/esm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:      (10000, 6), pos-ratio=0.500\n",
      "Validation set: (2000, 6),   pos-ratio=0.500\n",
      "Test1 set:      (2000, 6), pos-ratio=0.500\n",
      "Test2 set:      (10000, 6), pos-ratio=0.090\n",
      "\n",
      "All datasets successfully saved to 'data/medium_set'\n"
     ]
    }
   ],
   "source": [
    "# 1) Load the four “full” splits\n",
    "train_full  = pd.read_csv('../data/full_dataset/train_data.csv')\n",
    "val_full    = pd.read_csv('../data/full_dataset/validation_data.csv')\n",
    "test1_full  = pd.read_csv('../data/full_dataset/test1_data.csv')\n",
    "test2_full  = pd.read_csv('../data/full_dataset/test2_data.csv')\n",
    "\n",
    "# 2) Build a 10 000-example train set, 50/50\n",
    "n_train = 10000\n",
    "n_pos_train = n_neg_train = n_train // 2\n",
    "pos_train = train_full[train_full['isInteraction'] == 1]\\\n",
    "              .sample(n=n_pos_train, random_state=42)\n",
    "neg_train = train_full[train_full['isInteraction'] == 0]\\\n",
    "              .sample(n=n_neg_train, random_state=42)\n",
    "train_data = pd.concat([pos_train, neg_train])\\\n",
    "               .sample(frac=1, random_state=43)\\\n",
    "               .reset_index(drop=True)\n",
    "\n",
    "# 3) Build a “reasonable” validation set: 2 000 examples, 50/50\n",
    "n_val = 2000\n",
    "n_pos_val = n_neg_val = n_val // 2\n",
    "pos_val = val_full[val_full['isInteraction'] == 1]\\\n",
    "            .sample(n=n_pos_val, random_state=44)\n",
    "neg_val = val_full[val_full['isInteraction'] == 0]\\\n",
    "            .sample(n=n_neg_val, random_state=44)\n",
    "cv_data = pd.concat([pos_val, neg_val])\\\n",
    "            .sample(frac=1, random_state=45)\\\n",
    "            .reset_index(drop=True)\n",
    "cv_data['trainTest'] = 'validation'\n",
    "\n",
    "# 4) Build test1: 2 000 examples, 50/50\n",
    "n_test1 = 2000\n",
    "n_pos_test1 = n_neg_test1 = n_test1 // 2\n",
    "pos_test1 = test1_full[test1_full['isInteraction'] == 1]\\\n",
    "              .sample(n=n_pos_test1, random_state=46)\n",
    "neg_test1 = test1_full[test1_full['isInteraction'] == 0]\\\n",
    "              .sample(n=n_neg_test1, random_state=46)\n",
    "test1_data = pd.concat([pos_test1, neg_test1])\\\n",
    "               .sample(frac=1, random_state=47)\\\n",
    "               .reset_index(drop=True)\n",
    "\n",
    "# 5) Build test2: 10 000 examples, ~9% pos, ~91% neg\n",
    "n_test2 = 10000\n",
    "n_pos_test2 = int(n_test2 * 0.09)\n",
    "n_neg_test2 = n_test2 - n_pos_test2\n",
    "pos_test2 = test2_full[test2_full['isInteraction'] == 1]\\\n",
    "              .sample(n=n_pos_test2, random_state=48)\n",
    "neg_test2 = test2_full[test2_full['isInteraction'] == 0]\\\n",
    "              .sample(n=n_neg_test2, random_state=48)\n",
    "test2_data = pd.concat([pos_test2, neg_test2])\\\n",
    "               .sample(frac=1, random_state=49)\\\n",
    "               .reset_index(drop=True)\n",
    "\n",
    "# 6) Print class‐balance stats\n",
    "print(f\"Train set:      {train_data.shape}, pos-ratio={train_data['isInteraction'].mean():.3f}\")\n",
    "print(f\"Validation set: {cv_data.shape},   pos-ratio={cv_data['isInteraction'].mean():.3f}\")\n",
    "print(f\"Test1 set:      {test1_data.shape}, pos-ratio={test1_data['isInteraction'].mean():.3f}\")\n",
    "print(f\"Test2 set:      {test2_data.shape}, pos-ratio={test2_data['isInteraction'].mean():.3f}\")\n",
    "\n",
    "# 7) Save to disk\n",
    "out_dir = '../data/medium_set'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "for df, name in [\n",
    "    (train_data, 'train_data'),\n",
    "    (cv_data,    'validation_data'),\n",
    "    (test1_data, 'test1_data'),\n",
    "    (test2_data, 'test2_data'),\n",
    "]:\n",
    "    df.to_pickle(f'{out_dir}/{name}.pkl')\n",
    "    df.to_csv(f'{out_dir}/{name}.csv', index=False)\n",
    "\n",
    "print(\"\\nAll datasets successfully saved to 'data/medium_set'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprotID_A</th>\n",
       "      <th>uniprotID_B</th>\n",
       "      <th>isInteraction</th>\n",
       "      <th>trainTest</th>\n",
       "      <th>sequence_A</th>\n",
       "      <th>sequence_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q92529</td>\n",
       "      <td>Q9H6L4</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>MLPRTKYNRFRNDSVTSVDDLLHSLSVSGGGGKVSAARATPAAAPY...</td>\n",
       "      <td>MAQKPKVDPHVGRLGYLQALVTEFQETQSQDAKEQVLANLANFAYD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P09326</td>\n",
       "      <td>Q02446</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>MCSRGWDSCLALELLLLPLSLLVTSIQGHLVHMTVVSGSNVTLNIS...</td>\n",
       "      <td>MSDQKKEEEEEAAAAAAMATEGGKTSEPENNNKKPKTSGSQDSQPS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q4V328</td>\n",
       "      <td>Q9H2H9</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>MAQALSEEEFQRMQAQLLELRTNNYQLSDELRKNGVELTSLRQKVA...</td>\n",
       "      <td>MMHFKSGLELTELQNMTVPEDDNISNDSNDFTEVENGQINSKFISD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O95835</td>\n",
       "      <td>Q9NTJ5</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>MKRSEKPEGYRQMRPKTFPASNYTVSSRQMLQEIRESLRNLSKPSD...</td>\n",
       "      <td>MATAAYEQLKLHITPEKFYVEACDDGADDVLTIDRVSTEVTLAVKK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O95125</td>\n",
       "      <td>Q96JC9</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>MATAVEPEDQDLWEEEGILMVKLEDDFTCRPESVLQRDDPVLETSH...</td>\n",
       "      <td>MNGTANPLLDREEHCLRLGESFEKRPRASFHTIRYDFKPASIDTSC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>P78310</td>\n",
       "      <td>Q13094</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>MALLLCFVLLCGVVDFARSLSITTPEEMIEKAKGETAYLPCKFTLS...</td>\n",
       "      <td>MALRNVPFRSEVLGWDPDSLADYFKKLNYKDCEKAVKKYHIDGARF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Q5T7W7</td>\n",
       "      <td>Q9Y2D8</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>MPSSTSPDQGDDLENCILRFSDLDLKDMSLINPSSSLKAELDGSTK...</td>\n",
       "      <td>MGDWMTVTDPGLSSESKTISQYTSETKMSPSSLYSQQVLCSSIPLS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Q3KNW5</td>\n",
       "      <td>Q8WXI8</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>MRANCSSSSACPANSSEEELPVGLEVHGNLELVFTVVSTVMMGLLM...</td>\n",
       "      <td>MGLEKPQSKLEGGMHPQLIPSVIAVVFILLLSVCFIASCLVTHHNF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Q9UIG4</td>\n",
       "      <td>Q9Y224</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>MILNWKLLGILVLCLHTRGISGSEGHPSHPPAEDREEAGSPTLPQG...</td>\n",
       "      <td>MFRRKLTALDYHNPAGFNCKDETEFRNFIVWLEDQKIRHYKIEDRG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>O00560</td>\n",
       "      <td>Q5JR59</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>MSLYPSLEDLKVDKVIQAQTAFSANPANPAILSEASAPIPHDGNLY...</td>\n",
       "      <td>MSVPVAPKKSCYTQLRDNRNAARNNNESILSLGDTNANQIMLEVSS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     uniprotID_A uniprotID_B  isInteraction trainTest  \\\n",
       "0         Q92529      Q9H6L4              0     train   \n",
       "1         P09326      Q02446              0     train   \n",
       "2         Q4V328      Q9H2H9              0     train   \n",
       "3         O95835      Q9NTJ5              0     train   \n",
       "4         O95125      Q96JC9              1     train   \n",
       "...          ...         ...            ...       ...   \n",
       "9995      P78310      Q13094              0     train   \n",
       "9996      Q5T7W7      Q9Y2D8              1     train   \n",
       "9997      Q3KNW5      Q8WXI8              0     train   \n",
       "9998      Q9UIG4      Q9Y224              1     train   \n",
       "9999      O00560      Q5JR59              1     train   \n",
       "\n",
       "                                             sequence_A  \\\n",
       "0     MLPRTKYNRFRNDSVTSVDDLLHSLSVSGGGGKVSAARATPAAAPY...   \n",
       "1     MCSRGWDSCLALELLLLPLSLLVTSIQGHLVHMTVVSGSNVTLNIS...   \n",
       "2     MAQALSEEEFQRMQAQLLELRTNNYQLSDELRKNGVELTSLRQKVA...   \n",
       "3     MKRSEKPEGYRQMRPKTFPASNYTVSSRQMLQEIRESLRNLSKPSD...   \n",
       "4     MATAVEPEDQDLWEEEGILMVKLEDDFTCRPESVLQRDDPVLETSH...   \n",
       "...                                                 ...   \n",
       "9995  MALLLCFVLLCGVVDFARSLSITTPEEMIEKAKGETAYLPCKFTLS...   \n",
       "9996  MPSSTSPDQGDDLENCILRFSDLDLKDMSLINPSSSLKAELDGSTK...   \n",
       "9997  MRANCSSSSACPANSSEEELPVGLEVHGNLELVFTVVSTVMMGLLM...   \n",
       "9998  MILNWKLLGILVLCLHTRGISGSEGHPSHPPAEDREEAGSPTLPQG...   \n",
       "9999  MSLYPSLEDLKVDKVIQAQTAFSANPANPAILSEASAPIPHDGNLY...   \n",
       "\n",
       "                                             sequence_B  \n",
       "0     MAQKPKVDPHVGRLGYLQALVTEFQETQSQDAKEQVLANLANFAYD...  \n",
       "1     MSDQKKEEEEEAAAAAAMATEGGKTSEPENNNKKPKTSGSQDSQPS...  \n",
       "2     MMHFKSGLELTELQNMTVPEDDNISNDSNDFTEVENGQINSKFISD...  \n",
       "3     MATAAYEQLKLHITPEKFYVEACDDGADDVLTIDRVSTEVTLAVKK...  \n",
       "4     MNGTANPLLDREEHCLRLGESFEKRPRASFHTIRYDFKPASIDTSC...  \n",
       "...                                                 ...  \n",
       "9995  MALRNVPFRSEVLGWDPDSLADYFKKLNYKDCEKAVKKYHIDGARF...  \n",
       "9996  MGDWMTVTDPGLSSESKTISQYTSETKMSPSSLYSQQVLCSSIPLS...  \n",
       "9997  MGLEKPQSKLEGGMHPQLIPSVIAVVFILLLSVCFIASCLVTHHNF...  \n",
       "9998  MFRRKLTALDYHNPAGFNCKDETEFRNFIVWLEDQKIRHYKIEDRG...  \n",
       "9999  MSVPVAPKKSCYTQLRDNRNAARNNNESILSLGDTNANQIMLEVSS...  \n",
       "\n",
       "[10000 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Checking Individual Protein Frequencies in train_data ---\n",
      "Total unique proteins involved in any pair: 5589\n",
      "Found 3199 unique proteins that appear in more than one pair context.\n",
      "Displaying these proteins and their frequencies (how many pairs they are part of):\n",
      "  Protein ID: O76024, Appears in: 70 pairs\n",
      "  Protein ID: P05067, Appears in: 61 pairs\n",
      "  Protein ID: Q08379, Appears in: 59 pairs\n",
      "  Protein ID: A8MQ03, Appears in: 58 pairs\n",
      "  Protein ID: Q96HA8, Appears in: 52 pairs\n",
      "  Protein ID: Q7Z699, Appears in: 51 pairs\n",
      "  Protein ID: P60410, Appears in: 50 pairs\n",
      "  Protein ID: Q9NRD5, Appears in: 50 pairs\n",
      "  Protein ID: Q15323, Appears in: 48 pairs\n",
      "  Protein ID: O60333, Appears in: 44 pairs\n",
      "  Protein ID: Q0VD86, Appears in: 43 pairs\n",
      "  Protein ID: Q8TBB1, Appears in: 43 pairs\n",
      "  Protein ID: P60409, Appears in: 42 pairs\n",
      "  Protein ID: P61981, Appears in: 41 pairs\n",
      "  Protein ID: Q6A162, Appears in: 41 pairs\n",
      "  Protein ID: Q04864, Appears in: 40 pairs\n",
      "  Protein ID: P13473, Appears in: 39 pairs\n",
      "  Protein ID: O00560, Appears in: 38 pairs\n",
      "  Protein ID: O75031, Appears in: 38 pairs\n",
      "  Protein ID: P00533, Appears in: 36 pairs\n",
      "  ... and 3179 more proteins.\n",
      "\n",
      "--- End of Individual Protein Frequency Check ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# This code assumes 'train_data' DataFrame is already defined and populated.\n",
    "# If 'train_data' is not yet defined in the session, \n",
    "# you might need to re-run the cell that creates it first.\n",
    "# For example, if you loaded it from a file:\n",
    "# train_data = pd.read_csv('../data/medium_set/train_data.csv') \n",
    "# Or if it's from the notebook cell: make sure that cell has been executed.\n",
    "\n",
    "print(\"--- Checking Individual Protein Frequencies in train_data ---\")\n",
    "\n",
    "# Ensure the DataFrame and relevant columns exist\n",
    "if 'train_data' not in locals() or not isinstance(train_data, pd.DataFrame):\n",
    "    print(\"Error: 'train_data' DataFrame not found. Please ensure it is loaded.\")\n",
    "elif 'uniprotID_A' not in train_data.columns or 'uniprotID_B' not in train_data.columns:\n",
    "    print(\"Error: 'train_data' must contain 'uniprotID_A' and 'uniprotID_B' columns.\")\n",
    "else:\n",
    "    # Extract all protein IDs from both columns into a single list\n",
    "    all_proteins_in_pairs = list(train_data['uniprotID_A']) + list(train_data['uniprotID_B'])\n",
    "    \n",
    "    # Count the occurrences of each protein ID\n",
    "    protein_counts = Counter(all_proteins_in_pairs)\n",
    "    \n",
    "    # Filter for proteins that appear in more than one pair context\n",
    "    # (i.e., their count in the combined list is > 1)\n",
    "    proteins_in_multiple_pairs = {protein: count for protein, count in protein_counts.items() if count > 1}\n",
    "    \n",
    "    num_unique_proteins_total = len(protein_counts)\n",
    "    num_proteins_in_multiple = len(proteins_in_multiple_pairs)\n",
    "\n",
    "    print(f\"Total unique proteins involved in any pair: {num_unique_proteins_total}\")\n",
    "    \n",
    "    if num_proteins_in_multiple > 0:\n",
    "        print(f\"Found {num_proteins_in_multiple} unique proteins that appear in more than one pair context.\")\n",
    "        print(\"Displaying these proteins and their frequencies (how many pairs they are part of):\")\n",
    "        \n",
    "        # Sort for consistent display (e.g., by count descending, then by protein ID)\n",
    "        sorted_multiple_proteins = sorted(proteins_in_multiple_pairs.items(), key=lambda item: (-item[1], item[0]))\n",
    "        \n",
    "        # Displaying top N or all, depending on how many there are\n",
    "        display_limit = 20 \n",
    "        for i, (protein, count) in enumerate(sorted_multiple_proteins):\n",
    "            if i < display_limit:\n",
    "                print(f\"  Protein ID: {protein}, Appears in: {count} pairs\")\n",
    "            elif i == display_limit:\n",
    "                print(f\"  ... and {num_proteins_in_multiple - display_limit} more proteins.\")\n",
    "                break\n",
    "        if num_proteins_in_multiple <= display_limit:\n",
    "             print(f\"  (Displayed all {num_proteins_in_multiple} proteins occurring in multiple pairs)\")\n",
    "\n",
    "    else:\n",
    "        print(\"No protein appears in more than one pair context (each protein is unique to a single slot in a single pair, or appears only once across all pairs).\")\n",
    "\n",
    "print(\"\\n--- End of Individual Protein Frequency Check ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode proteins using ESM C model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to set memory fraction to 0.7 for GPU 0\n",
      "Call to set_per_process_memory_fraction for GPU 0 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 97541.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ESM-C] Loaded locally on cuda\n"
     ]
    }
   ],
   "source": [
    "from esm.models.esmc import ESMC\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    target_gpu_index_for_fraction = 0\n",
    "\n",
    "    print(f\"Attempting to set memory fraction to 0.7 for GPU {target_gpu_index_for_fraction}\")\n",
    "    torch.cuda.set_per_process_memory_fraction(0.7, device=target_gpu_index_for_fraction)\n",
    "    print(f\"Call to set_per_process_memory_fraction for GPU {target_gpu_index_for_fraction} completed.\")\n",
    "\n",
    "model = ESMC.from_pretrained(\"esmc_300m\").to(device)\n",
    "print(f\"[ESM-C] Loaded locally on {device}\")\n",
    "model_type = \"local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_protein_embedding(sequence):\n",
    "    \"\"\"\n",
    "    Get protein embedding for a given sequence using the loaded ESM model.\n",
    "    Optimized with torch.no_grad() for inference.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():  # Ensures no gradients are computed for model operations\n",
    "        protein = ESMProtein(sequence=sequence)\n",
    "        protein_tensor = model.encode(protein)  # Model inference step\n",
    "        logits_output = model.logits(           # Model inference step\n",
    "            protein_tensor,\n",
    "            LogitsConfig(sequence=False, return_embeddings=True)\n",
    "        )\n",
    "        # Get the per-protein representation by mean-pooling across sequence length\n",
    "        embedding = logits_output.embeddings\n",
    "    return embedding.cpu().numpy() # Move to CPU and convert to NumPy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm # Use tqdm.auto for better notebook compatibility\n",
    "\n",
    "def process_dataset(df, sample_size=None):\n",
    "    \"\"\"Process dataset to add embeddings for both protein sequences.\n",
    "    This function calls the optimized get_protein_embedding.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with protein sequences in 'sequence_A' and 'sequence_B' columns.\n",
    "        sample_size: Optional, number of examples to process (for testing).\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe to avoid modifying the original\n",
    "    if sample_size is not None and 0 < sample_size < len(df):\n",
    "        # Ensure sample_size is valid if provided\n",
    "        result_df = df.sample(n=sample_size, random_state=42).copy()\n",
    "        print(f\"Processing a sample of {len(result_df)} examples.\")\n",
    "    elif sample_size is not None and sample_size >= len(df):\n",
    "        print(f\"sample_size ({sample_size}) is >= DataFrame length ({len(df)}). Processing all examples.\")\n",
    "        result_df = df.copy()\n",
    "    else: # sample_size is None or 0 or invalid\n",
    "        result_df = df.copy()\n",
    "        print(f\"Processing all {len(result_df)} examples.\")\n",
    "\n",
    "    embeddings_A = []\n",
    "    embeddings_B = []\n",
    "\n",
    "    # Iterate through the selected DataFrame rows\n",
    "    # Ensure 'sequence_A' and 'sequence_B' columns exist\n",
    "    if 'sequence_A' not in result_df.columns or 'sequence_B' not in result_df.columns:\n",
    "        raise ValueError(\"DataFrame must contain 'sequence_A' and 'sequence_B' columns.\")\n",
    "\n",
    "    for i, row in tqdm(result_df.iterrows(), total=len(result_df), desc=\"Encoding proteins\"):\n",
    "        # Get embeddings for protein A and B using the optimized function\n",
    "        embedding_A = get_protein_embedding(row['sequence_A'])\n",
    "        embedding_B = get_protein_embedding(row['sequence_B'])\n",
    "\n",
    "        embeddings_A.append(embedding_A)\n",
    "        embeddings_B.append(embedding_B)\n",
    "\n",
    "    # Store the embeddings as new columns in the DataFrame\n",
    "    result_df['embedding_A'] = embeddings_A\n",
    "    result_df['embedding_B'] = embeddings_B\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Encoding train split ---\n",
      "Processing all 10000 examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding proteins: 100%|██████████| 10000/10000 [10:05<00:00, 16.51it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Encoding train split ---\")\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_pickle(\"../data/medium_set/train_data.pkl\")\n",
    "train_data_with_embeddings = process_dataset(df)\n",
    "train_data_with_embeddings.to_pickle('../data/medium_set/embeddings/train_data_with_embeddings.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Validation split\n",
    "if 'cuda' in str(device):\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"\\n--- Encoding validation split ---\")\n",
    "df = pd.read_pickle(\"../data/medium_set/validation_data.pkl\")\n",
    "val_data_with_embeddings = process_dataset(df)\n",
    "val_data_with_embeddings.to_pickle('../data/medium_set/embeddings/validation_data_with_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Test1 split\n",
    "if 'cuda' in str(device):\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"\\n--- Encoding test1 split ---\")\n",
    "df = pd.read_pickle(\"../data/medium_set/test1_data.pkl\", index_col=0)\n",
    "test1_data_with_embeddings = process_dataset(df)\n",
    "test1_data_with_embeddings.to_pickle('../data/medium_set/embeddings/test1_data_with_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(df, sample_size=None, device_to_check=None): # Added device_to_check\n",
    "    \"\"\"Process dataset to add embeddings for both protein sequences,\n",
    "    with GPU cache clearing to help manage memory.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with protein sequences in 'sequence_A' and 'sequence_B' columns.\n",
    "        sample_size: Optional, number of examples to process (for testing).\n",
    "        device_to_check: The torch.device being used (e.g., global 'device').\n",
    "                         This is needed to conditionally empty CUDA cache.\n",
    "    \"\"\"\n",
    "    if device_to_check is None:\n",
    "        # Fallback to a globally defined 'device' if not passed, or raise an error\n",
    "        # For this example, let's try to use a global 'device' if not provided,\n",
    "        # but passing it explicitly is better practice.\n",
    "        try:\n",
    "            # This line assumes 'device' is a global variable.\n",
    "            # If not, this will cause a NameError.\n",
    "            # It's better to require 'device_to_check' to be passed.\n",
    "            # For robustness, we should handle if 'device' isn't globally defined.\n",
    "            # Consider raising an error if device_to_check is None and no global 'device' is found.\n",
    "            # For now, let's assume 'device' is available if device_to_check is None\n",
    "            global device # This declares intent to use a global variable named 'device'\n",
    "            current_device_str = str(device)\n",
    "            if 'cuda' not in current_device_str:\n",
    "                 print(\"Warning: device_to_check not provided, and global 'device' is not CUDA. CUDA cache will not be cleared.\")\n",
    "        except NameError:\n",
    "            print(\"Warning: 'device_to_check' not provided and global 'device' not found. GPU cache clearing will be skipped.\")\n",
    "            current_device_str = \"cpu\" # Assume CPU if device is unknown\n",
    "    else:\n",
    "        current_device_str = str(device_to_check)\n",
    "\n",
    "\n",
    "    # --- DataFrame preparation (sampling) ---\n",
    "    if sample_size is not None and 0 < sample_size < len(df):\n",
    "        result_df = df.sample(n=sample_size, random_state=42).copy()\n",
    "        print(f\"Processing a sample of {len(result_df)} examples.\")\n",
    "    elif sample_size is not None and sample_size >= len(df):\n",
    "        print(f\"sample_size ({sample_size}) is >= DataFrame length ({len(df)}). Processing all examples.\")\n",
    "        result_df = df.copy()\n",
    "    else: # sample_size is None or 0 or invalid\n",
    "        result_df = df.copy()\n",
    "        print(f\"Processing all {len(result_df)} examples.\")\n",
    "\n",
    "    embeddings_A = []\n",
    "    embeddings_B = []\n",
    "\n",
    "    if 'sequence_A' not in result_df.columns or 'sequence_B' not in result_df.columns:\n",
    "        raise ValueError(\"DataFrame must contain 'sequence_A' and 'sequence_B' columns.\")\n",
    "\n",
    "    # --- Main processing loop ---\n",
    "    for i, row in tqdm(result_df.iterrows(), total=len(result_df), desc=\"Encoding proteins\"):\n",
    "        embedding_A_val = None\n",
    "        embedding_B_val = None\n",
    "        try:\n",
    "            # Get embeddings for protein A and B.\n",
    "            # Crucially, get_protein_embedding should use `torch.no_grad()` internally\n",
    "            # and return embeddings on the CPU (e.g., as NumPy arrays).\n",
    "            embedding_A_val = get_protein_embedding(row['sequence_A'])\n",
    "            embedding_B_val = get_protein_embedding(row['sequence_B'])\n",
    "\n",
    "        except Exception as e:\n",
    "            row_identifier = row.name if hasattr(row, 'name') and row.name is not None else f\"at numerical index {i}\"\n",
    "            print(f\"Error getting embedding for row {row_identifier}: {e}\")\n",
    "            # Embeddings will remain None and be appended as such.\n",
    "\n",
    "        finally:\n",
    "            embeddings_A.append(embedding_A_val)\n",
    "            embeddings_B.append(embedding_B_val)\n",
    "\n",
    "            # --- GPU Memory Optimization ---\n",
    "            # If running on GPU, clear the CUDA cache after processing each pair.\n",
    "            # This helps release memory that PyTorch's allocator might be holding onto\n",
    "            # but isn't actively used by tensors still in scope.\n",
    "            # This does NOT free memory of tensors that are still referenced.\n",
    "            # Calling this can have a small performance overhead.\n",
    "            if 'cuda' in current_device_str:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    # --- Store embeddings in DataFrame ---\n",
    "    result_df['embedding_A'] = embeddings_A\n",
    "    result_df['embedding_B'] = embeddings_B\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Current device from previous cells: {device if 'device' in locals() else 'device not defined yet'}\")\n",
    "\n",
    "if 'cuda' in str(device if 'device' in locals() else \"\"): # Check if the globally defined 'device' is CUDA\n",
    "    print(f\"Operating on CUDA device: {device}. Clearing cache before processing test2 split.\")\n",
    "    # This call to empty_cache() is a CUDA operation.\n",
    "    torch.cuda.empty_cache()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
    "\n",
    "print(\"\\n--- Encoding test2 split ---\")\n",
    "\n",
    "df_path = \"../data/medium_set/test2_data.pkl\"\n",
    "df = pd.read_pickle(df_path, index_col=0)\n",
    "print(f\"Loaded '{df_path}' with {len(df)} rows.\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    target_gpu_index_for_fraction = 0\n",
    "\n",
    "    print(f\"Attempting to set memory fraction to 0.7 for GPU {target_gpu_index_for_fraction} (Timing is critical and likely too late here).\")\n",
    "    torch.cuda.set_per_process_memory_fraction(0.7, device=target_gpu_index_for_fraction)\n",
    "    print(f\"Call to set_per_process_memory_fraction for GPU {target_gpu_index_for_fraction} completed.\")\n",
    "\n",
    "test2_data_with_embeddings = process_dataset(df)\n",
    "\n",
    "output_dir = '../data/medium_set/embeddings/'\n",
    "os.makedirs(output_dir, exist_ok=True) # Create the directory if it doesn't exist\n",
    "\n",
    "output_file_path = os.path.join(output_dir, 'test2_data_with_embeddings.pkl')\n",
    "test2_data_with_embeddings.to_pickle(output_file_path)\n",
    "print(f\"Processed data and saved embeddings to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "df = pd.read_csv('../data/medium_set/embeddings/test2_data_with_embeddings.csv')\n",
    "\n",
    "with open('../data/medium_set/embeddings/test2_data_with_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/medium_set/embeddings/test2_data_with_embeddings.pkl')\n",
    "print(df.head())\n",
    "print(df.dtypes)\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "print(type(df['embedding_A'][0]))\n",
    "print(type(df['embedding_B'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "df['embedding_A'] = df['embedding_A'].apply(lambda x: x.tolist())\n",
    "df.to_csv('fixed_embeddings.csv', index=False)\n",
    "df['embedding_B'] = df['embedding_B'].apply(lambda x: x.tolist())\n",
    "df.to_csv('fixed_embeddings.csv', index=False)\n",
    "# df['embedding_A'] = df['embedding_A'].apply(ast.literal_eval)\n",
    "# df['embedding_B'] = df['embedding_B'].apply(ast.literal_eval)\n",
    "\n",
    "embedding_A = np.vstack([np.array(x).squeeze() for x in df['embedding_A']])\n",
    "embedding_B = np.vstack([np.array(x).squeeze() for x in df['embedding_B']])\n",
    "\n",
    "print(embedding_A.shape)\n",
    "print(embedding_B.shape)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

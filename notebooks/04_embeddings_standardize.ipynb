{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the embeddings dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /home/richard/projects/CS182-Final-Project/src/data_process\n",
      "Changed to project root: /home/richard/projects/CS182-Final-Project\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "\n",
    "# Set the working directory to the project root\n",
    "# Get the current working directory and navigate to project root\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Current directory: {current_dir}\")\n",
    "\n",
    "# Find the CS182-Final-Project directory\n",
    "if 'CS182-Final-Project' in current_dir:\n",
    "    # If we're already in a subdirectory of the project, navigate to root\n",
    "    project_root = current_dir.split('CS182-Final-Project')[0] + 'CS182-Final-Project'\n",
    "else:\n",
    "    # If we're not in the project directory, assume we need to navigate\n",
    "    project_root = '/home/richard/projects/CS182-Final-Project'\n",
    "\n",
    "os.chdir(project_root)\n",
    "print(f\"Changed to project root: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/richard/projects/CS182-Final-Project\n",
      "Project root contents: ['results', 'experiments', '.gitignore', 'README.md', 'data', 'requirements.txt', '.git', 'docs', 'src']\n",
      "✓ Input file found: data/full_dataset/embeddings/embeddings_merged.pkl\n",
      "✓ File size: 11.23 GB\n",
      "Output will be saved to: data/full_dataset/embeddings/embeddings_standardized.pkl\n"
     ]
    }
   ],
   "source": [
    "# Define file paths (relative to project root)\n",
    "input_file = 'data/full_dataset/embeddings/embeddings_merged.pkl'\n",
    "output_file = 'data/full_dataset/embeddings/embeddings_standardized.pkl'\n",
    "\n",
    "# Verify we're in the correct directory\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Project root contents: {os.listdir('.')}\")\n",
    "\n",
    "# Verify input file exists\n",
    "if os.path.exists(input_file):\n",
    "    file_size = os.path.getsize(input_file) / (1024**3)  # Size in GB\n",
    "    print(f\"✓ Input file found: {input_file}\")\n",
    "    print(f\"✓ File size: {file_size:.2f} GB\")\n",
    "else:\n",
    "    print(f\"✗ Error: Input file not found at {input_file}\")\n",
    "    print(f\"Available files in data/full_dataset/embeddings/:\")\n",
    "    if os.path.exists('data/full_dataset/embeddings/'):\n",
    "        print(os.listdir('data/full_dataset/embeddings/'))\n",
    "    \n",
    "print(f\"Output will be saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings data...\n",
      "Data type: <class 'dict'>\n",
      "Number of proteins: 12026\n",
      "\n",
      "Sample protein ID: O14796\n",
      "Sample embedding type: <class 'torch.Tensor'>\n",
      "Sample embedding shape: torch.Size([134, 960])\n",
      "Sample embedding dtype: torch.float16\n",
      "Sample values (first 5): tensor([ 0.0030, -0.0006,  0.0012, -0.0003,  0.0016], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# Load and examine the embeddings data\n",
    "print(\"Loading embeddings data...\")\n",
    "with open(input_file, 'rb') as f:\n",
    "    embeddings_data = pickle.load(f)\n",
    "\n",
    "print(f\"Data type: {type(embeddings_data)}\")\n",
    "print(f\"Number of proteins: {len(embeddings_data)}\")\n",
    "\n",
    "# Examine a sample embedding\n",
    "sample_key = list(embeddings_data.keys())[0]\n",
    "sample_embedding = embeddings_data[sample_key]\n",
    "print(f\"\\nSample protein ID: {sample_key}\")\n",
    "print(f\"Sample embedding type: {type(sample_embedding)}\")\n",
    "print(f\"Sample embedding shape: {sample_embedding.shape}\")\n",
    "print(f\"Sample embedding dtype: {sample_embedding.dtype}\")\n",
    "print(f\"Sample values (first 5): {sample_embedding[0, :5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing embedding dimensions...\n",
      "Processing embeddings to collect statistics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing proteins: 100%|██████████| 12026/12026 [00:29<00:00, 404.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding dimension: 960\n",
      "Sequence length range: 26 - 1502\n",
      "Average sequence length: 522.11\n",
      "Total embedding values: 6,027,745,920\n"
     ]
    }
   ],
   "source": [
    "# Analyze embedding dimensions and compute statistics\n",
    "print(\"Analyzing embedding dimensions...\")\n",
    "\n",
    "# Collect all embeddings to compute global statistics\n",
    "all_embeddings = []\n",
    "sequence_lengths = []\n",
    "embedding_dim = None\n",
    "\n",
    "print(\"Processing embeddings to collect statistics...\")\n",
    "for protein_id, embedding in tqdm(embeddings_data.items(), desc=\"Processing proteins\"):\n",
    "    # Convert to numpy if it's a torch tensor\n",
    "    if isinstance(embedding, torch.Tensor):\n",
    "        embedding_np = embedding.cpu().numpy().astype(np.float32)\n",
    "    else:\n",
    "        embedding_np = embedding.astype(np.float32)\n",
    "    \n",
    "    sequence_lengths.append(embedding_np.shape[0])\n",
    "    if embedding_dim is None:\n",
    "        embedding_dim = embedding_np.shape[1]\n",
    "    \n",
    "    # Flatten the embedding to add to the collection\n",
    "    all_embeddings.append(embedding_np.flatten())\n",
    "\n",
    "print(f\"\\nEmbedding dimension: {embedding_dim}\")\n",
    "print(f\"Sequence length range: {min(sequence_lengths)} - {max(sequence_lengths)}\")\n",
    "print(f\"Average sequence length: {np.mean(sequence_lengths):.2f}\")\n",
    "print(f\"Total embedding values: {sum(len(emb) for emb in all_embeddings):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing global statistics for standardization...\n",
      "Computing statistics incrementally...\n",
      "Processed 100 embedding arrays...\n",
      "Processed 200 embedding arrays...\n",
      "Processed 300 embedding arrays...\n",
      "Processed 400 embedding arrays...\n",
      "Processed 500 embedding arrays...\n",
      "Processed 600 embedding arrays...\n",
      "Processed 700 embedding arrays...\n",
      "Processed 800 embedding arrays...\n",
      "Processed 900 embedding arrays...\n",
      "Processed 1000 embedding arrays...\n",
      "Processed 1100 embedding arrays...\n",
      "Processed 1200 embedding arrays...\n",
      "Processed 1300 embedding arrays...\n",
      "Processed 1400 embedding arrays...\n",
      "Processed 1500 embedding arrays...\n",
      "Processed 1600 embedding arrays...\n",
      "Processed 1700 embedding arrays...\n",
      "Processed 1800 embedding arrays...\n",
      "Processed 1900 embedding arrays...\n",
      "Processed 2000 embedding arrays...\n",
      "Processed 2100 embedding arrays...\n",
      "Processed 2200 embedding arrays...\n",
      "Processed 2300 embedding arrays...\n",
      "Processed 2400 embedding arrays...\n",
      "Processed 2500 embedding arrays...\n",
      "Processed 2600 embedding arrays...\n",
      "Processed 2700 embedding arrays...\n",
      "Processed 2800 embedding arrays...\n",
      "Processed 2900 embedding arrays...\n",
      "Processed 3000 embedding arrays...\n",
      "Processed 3100 embedding arrays...\n",
      "Processed 3200 embedding arrays...\n",
      "Processed 3300 embedding arrays...\n",
      "Processed 3400 embedding arrays...\n",
      "Processed 3500 embedding arrays...\n",
      "Processed 3600 embedding arrays...\n",
      "Processed 3700 embedding arrays...\n",
      "Processed 3800 embedding arrays...\n",
      "Processed 3900 embedding arrays...\n",
      "Processed 4000 embedding arrays...\n",
      "Processed 4100 embedding arrays...\n",
      "Processed 4200 embedding arrays...\n",
      "Processed 4300 embedding arrays...\n",
      "Processed 4400 embedding arrays...\n",
      "Processed 4500 embedding arrays...\n",
      "Processed 4600 embedding arrays...\n",
      "Processed 4700 embedding arrays...\n",
      "Processed 4800 embedding arrays...\n",
      "Processed 4900 embedding arrays...\n",
      "Processed 5000 embedding arrays...\n",
      "Processed 5100 embedding arrays...\n",
      "Processed 5200 embedding arrays...\n",
      "Processed 5300 embedding arrays...\n",
      "Processed 5400 embedding arrays...\n",
      "Processed 5500 embedding arrays...\n",
      "Processed 5600 embedding arrays...\n",
      "Processed 5700 embedding arrays...\n",
      "Processed 5800 embedding arrays...\n",
      "Processed 5900 embedding arrays...\n",
      "Processed 6000 embedding arrays...\n",
      "Processed 6100 embedding arrays...\n",
      "Processed 6200 embedding arrays...\n",
      "Processed 6300 embedding arrays...\n",
      "Processed 6400 embedding arrays...\n",
      "Processed 6500 embedding arrays...\n",
      "Processed 6600 embedding arrays...\n",
      "Processed 6700 embedding arrays...\n",
      "Processed 6800 embedding arrays...\n",
      "Processed 6900 embedding arrays...\n",
      "Processed 7000 embedding arrays...\n",
      "Processed 7100 embedding arrays...\n",
      "Processed 7200 embedding arrays...\n",
      "Processed 7300 embedding arrays...\n",
      "Processed 7400 embedding arrays...\n",
      "Processed 7500 embedding arrays...\n",
      "Processed 7600 embedding arrays...\n",
      "Processed 7700 embedding arrays...\n",
      "Processed 7800 embedding arrays...\n",
      "Processed 7900 embedding arrays...\n",
      "Processed 8000 embedding arrays...\n",
      "Processed 8100 embedding arrays...\n",
      "Processed 8200 embedding arrays...\n",
      "Processed 8300 embedding arrays...\n",
      "Processed 8400 embedding arrays...\n",
      "Processed 8500 embedding arrays...\n",
      "Processed 8600 embedding arrays...\n",
      "Processed 8700 embedding arrays...\n",
      "Processed 8800 embedding arrays...\n",
      "Processed 8900 embedding arrays...\n",
      "Processed 9000 embedding arrays...\n",
      "Processed 9100 embedding arrays...\n",
      "Processed 9200 embedding arrays...\n",
      "Processed 9300 embedding arrays...\n",
      "Processed 9400 embedding arrays...\n",
      "Processed 9500 embedding arrays...\n",
      "Processed 9600 embedding arrays...\n",
      "Processed 9700 embedding arrays...\n",
      "Processed 9800 embedding arrays...\n",
      "Processed 9900 embedding arrays...\n",
      "Processed 10000 embedding arrays...\n",
      "Processed 10100 embedding arrays...\n",
      "Processed 10200 embedding arrays...\n",
      "Processed 10300 embedding arrays...\n",
      "Processed 10400 embedding arrays...\n",
      "Processed 10500 embedding arrays...\n",
      "Processed 10600 embedding arrays...\n",
      "Processed 10700 embedding arrays...\n",
      "Processed 10800 embedding arrays...\n",
      "Processed 10900 embedding arrays...\n",
      "Processed 11000 embedding arrays...\n",
      "Processed 11100 embedding arrays...\n",
      "Processed 11200 embedding arrays...\n",
      "Processed 11300 embedding arrays...\n",
      "Processed 11400 embedding arrays...\n",
      "Processed 11500 embedding arrays...\n",
      "Processed 11600 embedding arrays...\n",
      "Processed 11700 embedding arrays...\n",
      "Processed 11800 embedding arrays...\n",
      "Processed 11900 embedding arrays...\n",
      "Processed 12000 embedding arrays...\n",
      "\n",
      "Global statistics:\n",
      "Mean: 0.000402\n",
      "Standard deviation: 0.042776\n",
      "Min value: -1.302734\n",
      "Max value: 1.260742\n",
      "Total values: 6,027,745,920\n",
      "\n",
      "Statistics computed without memory issues.\n"
     ]
    }
   ],
   "source": [
    "# Compute global statistics for standardization (memory-efficient)\n",
    "print(\"Computing global statistics for standardization...\")\n",
    "\n",
    "# Initialize variables for incremental computation\n",
    "total_sum = 0.0\n",
    "total_sum_sq = 0.0\n",
    "total_count = 0\n",
    "min_val = float('inf')\n",
    "max_val = float('-inf')\n",
    "\n",
    "print(\"Computing statistics incrementally...\")\n",
    "for i, embeddings in enumerate(all_embeddings):\n",
    "    # Convert to numpy array if needed\n",
    "    if not isinstance(embeddings, np.ndarray):\n",
    "        embeddings = np.array(embeddings)\n",
    "    \n",
    "    # Update statistics\n",
    "    total_sum += np.sum(embeddings)\n",
    "    total_sum_sq += np.sum(embeddings ** 2)\n",
    "    total_count += embeddings.size\n",
    "    min_val = min(min_val, np.min(embeddings))\n",
    "    max_val = max(max_val, np.max(embeddings))\n",
    "    \n",
    "    if (i + 1) % 100 == 0:  # Progress indicator\n",
    "        print(f\"Processed {i + 1} embedding arrays...\")\n",
    "\n",
    "# Compute final statistics\n",
    "global_mean = total_sum / total_count\n",
    "global_std = np.sqrt((total_sum_sq / total_count) - (global_mean ** 2))\n",
    "\n",
    "print(f\"\\nGlobal statistics:\")\n",
    "print(f\"Mean: {global_mean:.6f}\")\n",
    "print(f\"Standard deviation: {global_std:.6f}\")\n",
    "print(f\"Min value: {min_val:.6f}\")\n",
    "print(f\"Max value: {max_val:.6f}\")\n",
    "print(f\"Total values: {total_count:,}\")\n",
    "\n",
    "print(\"\\nStatistics computed without memory issues.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying standardization to all embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standardizing embeddings: 100%|██████████| 12026/12026 [02:16<00:00, 88.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Standardization complete!\n",
      "Number of standardized embeddings: 12026\n",
      "\n",
      "Sample standardized embedding:\n",
      "Shape: (134, 960)\n",
      "Mean: -0.000438\n",
      "Std: 0.985250\n",
      "Min: -14.494860\n",
      "Max: 18.379944\n"
     ]
    }
   ],
   "source": [
    "# Apply standardization to all embeddings (minimal memory)\n",
    "print(\"Applying standardization to all embeddings...\")\n",
    "\n",
    "os.makedirs(\"standardized_embeddings\", exist_ok=True)\n",
    "\n",
    "processed_count = 0\n",
    "sample_key = None\n",
    "sample_standardized = None\n",
    "\n",
    "for protein_id, embedding in tqdm(embeddings_data.items(), desc=\"Standardizing embeddings\"):\n",
    "    # Convert to numpy if it's a torch tensor\n",
    "    if isinstance(embedding, torch.Tensor):\n",
    "        embedding_np = embedding.cpu().numpy().astype(np.float32)\n",
    "    else:\n",
    "        embedding_np = embedding.astype(np.float32)\n",
    "    \n",
    "    # Apply z-score standardization: (x - mean) / std\n",
    "    standardized_embedding = (embedding_np - global_mean) / global_std\n",
    "    \n",
    "    # Save immediately and clear from memory\n",
    "    np.save(f\"standardized_embeddings/{protein_id}.npy\", standardized_embedding)\n",
    "    \n",
    "    # Keep first one for verification (copy to avoid reference)\n",
    "    if sample_key is None:\n",
    "        sample_key = protein_id\n",
    "        sample_standardized = standardized_embedding.copy()\n",
    "    \n",
    "    processed_count += 1\n",
    "    \n",
    "    # Clear variables\n",
    "    del embedding_np, standardized_embedding\n",
    "    \n",
    "    # Periodic garbage collection\n",
    "    if processed_count % 50 == 0:\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"\\nStandardization complete!\")\n",
    "print(f\"Number of standardized embeddings: {processed_count}\")\n",
    "\n",
    "print(f\"\\nSample standardized embedding:\")\n",
    "print(f\"Shape: {sample_standardized.shape}\")\n",
    "print(f\"Mean: {np.mean(sample_standardized):.6f}\")\n",
    "print(f\"Std: {np.std(sample_standardized):.6f}\")\n",
    "print(f\"Min: {np.min(sample_standardized):.6f}\")\n",
    "print(f\"Max: {np.max(sample_standardized):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing comprehensive memory cleanup...\n",
      "Cleaned up variables: ['embeddings_data', 'all_embeddings', 'sample_standardized', 'embedding']\n",
      "Current memory usage: 26562.4 MB\n",
      "Closed all matplotlib figures\n",
      "Memory cleanup completed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Memory cleanup before verification\n",
    "print(\"Performing comprehensive memory cleanup...\")\n",
    "\n",
    "# List of variables that might be consuming memory\n",
    "variables_to_clean = [\n",
    "    'embeddings_data', 'all_embeddings', 'standardized_embeddings', \n",
    "    'standardized_embedding', 'embedding_np', 'standardized_embedding_var',\n",
    "    'sample_standardized', 'current_batch', 'batch_embeddings',\n",
    "    'all_embeddings', 'final_dict', 'embedding', 'flat_embedding',\n",
    "    'chunk_values', 'sample_values', 'sample_concat'\n",
    "]\n",
    "\n",
    "# Clean up variables if they exist\n",
    "cleaned_vars = []\n",
    "for var_name in variables_to_clean:\n",
    "    if var_name in globals():\n",
    "        del globals()[var_name]\n",
    "        cleaned_vars.append(var_name)\n",
    "    if var_name in locals():\n",
    "        del locals()[var_name]\n",
    "        cleaned_vars.append(var_name)\n",
    "\n",
    "if cleaned_vars:\n",
    "    print(f\"Cleaned up variables: {cleaned_vars}\")\n",
    "\n",
    "# Force garbage collection multiple times\n",
    "import gc\n",
    "for i in range(3):\n",
    "    collected = gc.collect()\n",
    "    if collected > 0:\n",
    "        print(f\"Garbage collection round {i+1}: freed {collected} objects\")\n",
    "\n",
    "# Check memory usage if psutil is available\n",
    "try:\n",
    "    import psutil\n",
    "    import os\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "    print(f\"Current memory usage: {memory_mb:.1f} MB\")\n",
    "except ImportError:\n",
    "    print(\"psutil not available for memory monitoring\")\n",
    "\n",
    "# Clear any matplotlib figures if they exist\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.close('all')\n",
    "    print(\"Closed all matplotlib figures\")\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Clear CUDA cache if using PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Cleared CUDA cache\")\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "print(\"Memory cleanup completed.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying global standardization properties...\n",
      "Computing verification statistics on sample of 1000 proteins...\n",
      "\n",
      "Verification on sample of 1000 proteins:\n",
      "Sample mean: 0.000037 (should be close to 0)\n",
      "Sample std: 1.001788 (should be close to 1)\n",
      "Sample min: -28.569386\n",
      "Sample max: 29.463774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify global standardization properties (ultra memory-efficient)\n",
    "print(\"Verifying global standardization properties...\")\n",
    "\n",
    "# Get list of saved embedding files\n",
    "import glob\n",
    "embedding_files = glob.glob(\"standardized_embeddings/*.npy\")\n",
    "sample_size = min(1000, len(embedding_files))  # Sample up to 1000 proteins\n",
    "\n",
    "# Use online/streaming statistics computation\n",
    "import random\n",
    "\n",
    "# Randomly sample protein files for verification\n",
    "sampled_files = random.sample(embedding_files, sample_size) if len(embedding_files) > sample_size else embedding_files\n",
    "\n",
    "total_sum = 0.0\n",
    "total_sum_sq = 0.0\n",
    "total_count = 0\n",
    "min_val = float('inf')\n",
    "max_val = float('-inf')\n",
    "\n",
    "print(f\"Computing verification statistics on sample of {len(sampled_files)} proteins...\")\n",
    "\n",
    "for file_path in sampled_files:\n",
    "    # Load embedding from file\n",
    "    embedding = np.load(file_path)\n",
    "    \n",
    "    # Process in small chunks to avoid memory spikes\n",
    "    flat_embedding = embedding.flatten()\n",
    "    \n",
    "    total_sum += np.sum(flat_embedding)\n",
    "    total_sum_sq += np.sum(flat_embedding ** 2)\n",
    "    total_count += flat_embedding.size\n",
    "    min_val = min(min_val, np.min(flat_embedding))\n",
    "    max_val = max(max_val, np.max(flat_embedding))\n",
    "    \n",
    "    # Clean up\n",
    "    del embedding, flat_embedding\n",
    "\n",
    "# Compute final verification statistics\n",
    "sample_mean = total_sum / total_count\n",
    "sample_std = np.sqrt((total_sum_sq / total_count) - (sample_mean ** 2))\n",
    "\n",
    "print(f\"\\nVerification on sample of {len(sampled_files)} proteins:\")\n",
    "print(f\"Sample mean: {sample_mean:.6f} (should be close to 0)\")\n",
    "print(f\"Sample std: {sample_std:.6f} (should be close to 1)\")\n",
    "print(f\"Sample min: {min_val:.6f}\")\n",
    "print(f\"Sample max: {max_val:.6f}\")\n",
    "\n",
    "# Clean up\n",
    "del sampled_files\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving standardized embeddings to data/full_dataset/embeddings/embeddings_standardized.pkl using streaming...\n",
      "Found 12026 standardized embedding files...\n",
      "Processing embeddings in small chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 602/602 [02:01<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving final pickle file...\n",
      "\n",
      "Standardized embeddings saved successfully!\n",
      "Output file: data/full_dataset/embeddings/embeddings_standardized.pkl\n",
      "Output file size: 22.46 GB\n",
      "Total embeddings saved: 12026\n",
      "\n",
      "Cleaning up individual embedding files...\n",
      "✓ Individual embedding files cleaned up successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save standardized embeddings (ultra memory-efficient streaming)\n",
    "print(f\"Saving standardized embeddings to {output_file} using streaming...\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "# Get all embedding files\n",
    "embedding_files = glob.glob(\"standardized_embeddings/*.npy\")\n",
    "total_files = len(embedding_files)\n",
    "print(f\"Found {total_files} standardized embedding files...\")\n",
    "\n",
    "# Use a temporary file approach to build the pickle incrementally\n",
    "import tempfile\n",
    "temp_file = output_file + \".tmp\"\n",
    "\n",
    "try:\n",
    "    # Create dictionary in chunks and save incrementally\n",
    "    final_dict = {}\n",
    "    chunk_size = 20  # Very small chunks\n",
    "    \n",
    "    print(\"Processing embeddings in small chunks...\")\n",
    "    for i in tqdm(range(0, total_files, chunk_size), desc=\"Processing chunks\"):\n",
    "        chunk_files = embedding_files[i:i+chunk_size]\n",
    "        \n",
    "        # Process tiny chunk\n",
    "        for file_path in chunk_files:\n",
    "            protein_id = os.path.basename(file_path).replace('.npy', '')\n",
    "            embedding = np.load(file_path)\n",
    "            final_dict[protein_id] = embedding\n",
    "            \n",
    "            # Immediate cleanup\n",
    "            del embedding\n",
    "        \n",
    "        # Force garbage collection every chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Save final result\n",
    "    print(\"Saving final pickle file...\")\n",
    "    with open(temp_file, 'wb') as f:\n",
    "        pickle.dump(final_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    # Move temp file to final location\n",
    "    import shutil\n",
    "    shutil.move(temp_file, output_file)\n",
    "    \n",
    "    # Verify the saved file\n",
    "    if os.path.exists(output_file):\n",
    "        output_size = os.path.getsize(output_file) / (1024**3)  # Size in GB\n",
    "        print(f\"\\nStandardized embeddings saved successfully!\")\n",
    "        print(f\"Output file: {output_file}\")\n",
    "        print(f\"Output file size: {output_size:.2f} GB\")\n",
    "        print(f\"Total embeddings saved: {len(final_dict)}\")\n",
    "        \n",
    "        # Clean up individual files after successful save\n",
    "        print(\"\\nCleaning up individual embedding files...\")\n",
    "        shutil.rmtree(\"standardized_embeddings\")\n",
    "        print(\"✓ Individual embedding files cleaned up successfully.\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nError: Failed to save file at {output_file}\")\n",
    "        print(\"Individual files preserved for safety.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during processing: {e}\")\n",
    "    print(\"Individual files preserved for safety.\")\n",
    "    # Clean up temp file if it exists\n",
    "    if os.path.exists(temp_file):\n",
    "        os.remove(temp_file)\n",
    "    \n",
    "finally:\n",
    "    # Clean up memory\n",
    "    if 'final_dict' in locals():\n",
    "        del final_dict\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading standardized embeddings...\n",
      "Number of proteins: 12026\n",
      "\n",
      "Standardized embeddings verification (sample of 100 proteins):\n",
      "Mean: -0.000497 (should be close to 0)\n",
      "Std: 1.003686 (should be close to 1)\n",
      "Min: -28.067131\n",
      "Max: 25.947996\n"
     ]
    }
   ],
   "source": [
    "# Simple verification of standardized embeddings\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load the standardized embeddings\n",
    "print(\"Loading standardized embeddings...\")\n",
    "with open('data/full_dataset/embeddings/embeddings_standardized.pkl', 'rb') as f:\n",
    "    standardized_data = pickle.load(f)\n",
    "\n",
    "print(f\"Number of proteins: {len(standardized_data)}\")\n",
    "\n",
    "# Sample a subset to check statistics (to avoid memory issues)\n",
    "sample_size = min(100, len(standardized_data))\n",
    "protein_ids = list(standardized_data.keys())[:sample_size]\n",
    "\n",
    "all_values = []\n",
    "for protein_id in protein_ids:\n",
    "    embedding = standardized_data[protein_id]\n",
    "    all_values.extend(embedding.flatten())\n",
    "\n",
    "all_values = np.array(all_values)\n",
    "\n",
    "print(f\"\\nStandardized embeddings verification (sample of {sample_size} proteins):\")\n",
    "print(f\"Mean: {np.mean(all_values):.6f} (should be close to 0)\")\n",
    "print(f\"Std: {np.std(all_values):.6f} (should be close to 1)\")\n",
    "print(f\"Min: {np.min(all_values):.6f}\")\n",
    "print(f\"Max: {np.max(all_values):.6f}\")\n",
    "\n",
    "# Clean up\n",
    "del standardized_data, all_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
